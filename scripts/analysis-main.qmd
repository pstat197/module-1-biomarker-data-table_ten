---
title: "analysis-main"
format: html
engine: knitr
jupyter: python3
---

### Question 1
What do you imagine is the reason for log-transforming the protein levels in biomarker-raw.csv? (Hint: look at the distribution of the raw values for a sample of proteins.)

```{r setup, include=FALSE}
library(reticulate)
use_virtualenv("~/.virtualenvs/r-reticulate", required = TRUE)

required_pkgs <- c("pandas", "numpy", "random", "math", "matplotlib", "scikit-learn")

for (pkg in required_pkgs) {
  if (!py_module_available(pkg)) {
    message(sprintf("Installing missing Python package: %s", pkg))
    py_install(pkg)
  }
}

py_config()
```



```{python}
# import dependencies
import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
import math
from sklearn.preprocessing import StandardScaler


# Question 1


# Get var names
var_names = pd.read_csv(
   "https://raw.githubusercontent.com/pstat197/module-1-biomarker-data-table_ten/refs/heads/main/data/biomarker-raw.csv",
   header=None,
   nrows=2
).iloc[:, 2:].T
var_names.columns = ["name", "abbreviation"]
var_names.dropna(inplace=True)


# Get data
biomarker = pd.read_csv(
   "https://raw.githubusercontent.com/pstat197/module-1-biomarker-data-table_ten/refs/heads/main/data/biomarker-raw.csv",
   skiprows=2,
   dtype={"Group": "string"},
   na_values=["-", ""]
)


# drop 2nd col
biomarker.drop(biomarker.columns[1], axis=1, inplace=True)


# assign col names
biomarker.columns = ["group"] + list(var_names["abbreviation"]) + ["ados"]


# drop nas
biomarker = biomarker[biomarker["group"].notna() & (biomarker["group"] != "")].copy()


# reorder cols
cols = [c for c in biomarker.columns if c not in ["group", "ados"]]
biomarker = biomarker[["group", "ados"] + cols]


biomarker


# Sample 3 random proteins
proteins = biomarker.columns
# Exclude the 'Group' column as it is not numeric
numeric_proteins = proteins.drop('group')
random_proteins = random.sample(list(numeric_proteins), 3)


print(f"Randomly selected proteins:")
for i, protein in enumerate(random_proteins):
   print(f"\t {i+1}. {protein}")




random_protein_levels = biomarker[random_proteins]


for protein in random_proteins:
   plt.figure()
   random_protein_levels[protein].plot(kind="hist", title=protein)


# perform log transform
group_col = biomarker["group"]
biomarker.drop(columns=["group"], inplace=True)
biomarker = np.log(biomarker)
biomarker.insert(0, "group", group_col)


# plot transformed previously selected proteins for comparison


random_protein_levels = biomarker[random_proteins]


for protein in random_proteins:
   plt.figure()
   random_protein_levels[protein].plot(kind="hist", title=protein)


# Drop group col since its non-numeric
numeric_biomarker = biomarker.drop(columns=['group'])


outlier_counts_per_index = {}
```

### Question 2
Temporarily remove the outlier trimming from preprocessing and do some exploratory analysis of the outlying values. Are there specific subjects (not values) that seem to be outliers? If so, are outliers more frequent in one group or the other? (Hint: consider tabulating the number of outlying values per subject.)

```{python}
# Detect outliers using the 1.5 IQR rule
for col in numeric_biomarker.columns:
 Q1 = numeric_biomarker[col].quantile(0.25)
 Q3 = numeric_biomarker[col].quantile(0.75)
 IQR = Q3 - Q1


 lower_bound = Q1 - 1.5 * IQR
 upper_bound = Q3 + 1.5 * IQR


 # Find outlier indices for the current col
 column_outlier_indices = numeric_biomarker[(numeric_biomarker[col] < lower_bound) | (numeric_biomarker[col] > upper_bound)].index


 # Increment the count for outlier indices
 for index in column_outlier_indices:
   outlier_counts_per_index[index] = outlier_counts_per_index.get(index, 0) + 1


# Sort indices by outlier count
sorted_outliers = sorted(outlier_counts_per_index.items(), key=lambda item: item[1], reverse=True)


# Get the top 10 indices w/ outlier counts
indices = []
counts = []
group = []
for index, count in sorted_outliers[:10]:
 indices.append(index)
 counts.append(count)
 group.append(biomarker.loc[index, 'group'])


pd.DataFrame({
   'index': indices,
   'group': group,
   'outliers': counts
})


ASD_outliers = 0
TD_outliers = 0


for index, count in sorted_outliers:
 if biomarker.loc[index, 'group'] == 'ASD':
   ASD_outliers += count
 else:
   TD_outliers += count


pd.DataFrame(
   {'ASD': [ASD_outliers],
    'TD': [TD_outliers]}
)
```

### Question 3
Experiment with the following modifications:

a) Repeat the analysis but carry out the entire selection procedure on a training partition -- in other words, set aside some testing data at the very beginning and don't use it until you are evaluating accuracy at the very end

```{r}
library(tidyverse)
library(infer)
library(randomForest)
library(tidymodels)
library(modelr)
library(yardstick)
load('../data/biomarker-clean.RData')
set.seed(101422)


# pre-split the data so analyses are performed on the training set exclusively
biomarker_split <- biomarker_clean %>% initial_split(prop = 0.8)
train_df <- training(biomarker_split)
test_df <- testing(biomarker_split)

## MULTIPLE TESTING
####################

# function to compute tests
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- train_df %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins
proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)

## RANDOM FOREST
##################

# store predictors and response separately
predictors <- train_df %>%
  select(-c(group, ados))

response <- train_df %>% pull(group) %>% factor()

# fit RF
set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

# check errors
rf_out$confusion

# compute importance scores
proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)

## LOGISTIC REGRESSION
#######################

# select subset of interest
proteins_sstar <- intersect(proteins_s1, proteins_s2)

biomarker_sstar <- train_df %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = biomarker_sstar, 
           family = 'binomial')


# Testing Section

# biomarker_sstar for testing evaluation set
test_eval <- test_df %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = factor(group, levels = c("TD","ASD"))) %>%
  select(-group)


# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy, roc_auc)

# metric set
test_eval %>%
  add_predictions(fit, type = 'response') %>%
  mutate(pred_class = factor(pred > 0.5,
                             levels = c(FALSE, TRUE),
                             labels = c("TD","ASD"))) %>%
  class_metrics(estimate = pred_class,
                truth = class,
                event_level = 'second',pred) |> knitr::kable()
```

b)Choose a larger number (more than ten) of top predictive proteins using each selection method
```{r}
# Compare top 10, 15, 20, 25, 30, 35, 40 proteins
# Function: top proteins by t-test
get_top_ttest <- function(data, n_top = 10){
  ttests_out <- data %>%
    select(-ados) %>%
    pivot_longer(-group, names_to = 'protein', values_to = 'level') %>%
    nest(data = c(level, group)) %>%
    mutate(ttest = map(data, ~ t_test(.x, formula = level ~ group, 
                                      order = c('ASD','TD'), 
                                      alternative = 'two-sided'))) %>%
    unnest(ttest) %>%
    arrange(p_value) %>%
    mutate(m = n(),
           hm = log(m) + 1/(2*m) - digamma(1),
           rank = row_number(),
           p.adj = m*hm*p_value/rank)
  
  proteins <- ttests_out %>% slice_min(p.adj, n = n_top) %>% pull(protein)
  return(proteins)
}

# Function: top proteins by RF
get_top_rf <- function(data, n_top = 10){
  predictors <- data %>% select(-c(group, ados))
  response <- factor(data$group)
  set.seed(101422)
  rf_out <- randomForest(x = predictors, y = response, ntree = 1000, importance = TRUE)
  proteins <- rf_out$importance %>% 
    as_tibble() %>%
    mutate(protein = rownames(rf_out$importance)) %>%
    slice_max(MeanDecreaseGini, n = n_top) %>%
    pull(protein)
  return(proteins)
}

# Function: logistic regression & metrics
get_logreg_metrics <- function(data, n_top){
  # get top proteins
  proteins_star <- intersect(get_top_ttest(data, n_top), get_top_rf(data, n_top))
  
  # subset data
  biomarker_sstar <- data %>%
    select(group, any_of(proteins_star)) %>%
    mutate(class = (group == 'ASD')) %>%
    select(-group)
  
  # train/test split
  set.seed(101422)
  biomarker_split <- initial_split(biomarker_sstar, prop = 0.8)
  
  # fit logistic regression
  fit <- glm(class ~ ., data = training(biomarker_split), family = 'binomial')
  
  # predict & evaluate
  results <- testing(biomarker_split) %>%
    add_predictions(fit, type = "response") %>%
    mutate(pred_class = factor(pred > 0.5, levels = c(FALSE, TRUE)),
           class = factor(class, levels = c(FALSE, TRUE)))
  
  metrics_res <- metric_set(sensitivity, specificity, accuracy)
  metrics_df <- metrics_res(results, truth = class, estimate = pred_class)
  
  # add N info
  metrics_df$top_proteins <- n_top
  
  return(metrics_df)
}

# Run comparison: 10, 15, 20, 25, 30, 35, 40 proteins
top_list <- c(10, 15, 20, 25, 30, 35, 40)
all_results <- map_dfr(top_list, ~ get_logreg_metrics(biomarker_clean, .x))

# Print comparison table
all_results %>%
  select(top_proteins, .metric, .estimate) %>%
  pivot_wider(names_from = .metric, values_from = .estimate) %>%
  arrange(top_proteins)

```

c) Use a fuzzy intersection instead of a hard intersection to combine the sets of top predictive proteins across selection methods

```{r}

## MULTIPLE TESTING
####################

# function to compute tests
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- biomarker_clean %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins
proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)




## RANDOM FOREST
##################

# store predictors and response separately
predictors <- biomarker_clean %>%
  select(-c(group, ados))

response <- biomarker_clean %>% pull(group) %>% factor()

# fit RF
set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)


# compute importance scores
proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)
## CORRELATION
#######################

corr_out <- biomarker_clean %>%
  select(-ados) %>%
  mutate(group = ifelse(group == "ASD", 1, 0)) %>%
  summarise(across(-group, ~ cor(.x, group))) %>%
  pivot_longer(everything(), names_to = "protein", values_to = "corr") %>%
  arrange(desc(abs(corr)))

proteins_s3 <- corr_out %>%
  slice_head(n = 10)

## RANK PROTEINS FROM EACH METHOD
#######################

rank_ttest <- ttests_out %>%
  arrange(p.adj) %>%
  mutate(rank_ttest = row_number()) %>%
  select(protein, rank_ttest)

rank_rf <- rf_out$importance %>%
  as_tibble(rownames = "protein") %>%
  arrange(desc(MeanDecreaseGini)) %>%
  mutate(rank_rf = row_number()) %>%
  select(protein, rank_rf)

rank_corr <- corr_out %>%
  mutate(rank_corr = rank(-abs(corr))) %>%
  select(protein, rank_corr)

## FIND FUZZY AND HARD INTERSECTION
#######################

fuzzy_ranks <- full_join(rank_ttest, rank_rf, by = "protein") %>%
  full_join(rank_corr, by = "protein") %>%
  mutate(mean_rank = rowMeans(across(starts_with("rank")))) %>%
  arrange(mean_rank)

proteins_fuzzy <- fuzzy_ranks %>%
  slice_min(mean_rank, n = 10) %>%
  pull(protein)
proteins_fuzzy



```

```{r}

# Define logistic regression function
logistic <- function(data, model_name, seed = 101422, prop = 0.8) {
  # Partition into training and test set
  set.seed(seed)
  split <- data %>% initial_split(prop = prop)
  
  # Fit logistic regression model
  fit <- glm(class ~ ., data = training(split), family = 'binomial')
  
  # Make predictions and compute metrics
  preds <- testing(split) %>%
    add_predictions(fit, type = 'response') %>%
    mutate(
      pred_class = factor(ifelse(pred > 0.5, "TRUE", "FALSE"), levels = c("FALSE", "TRUE")),
      class = factor(class, levels = c(FALSE, TRUE)),
      model = model_name
    )
  
  class_metrics <- metric_set(sensitivity, specificity, accuracy, roc_auc)
  metrics <- preds %>%
    class_metrics(truth = class, estimate = pred_class, pred, event_level = "second")
  
  list(preds = preds, fit = fit, metrics = metrics)
}

# Prepare datasets
biomarker_fuzzy <- biomarker_clean %>%
  select(group, any_of(proteins_fuzzy)) %>%
  mutate(class = (group == "ASD")) %>%
  select(-group)


# Run models
res_fuzzy <- logistic(biomarker_fuzzy, model_name = "Fuzzy Intersection")


# Combine predictions and compute summary metrics
all_preds <- bind_rows(res_fuzzy$preds) %>%
  mutate(pred = as.numeric(pred))

all_metrics <- all_preds %>%
  group_by(model) %>%
  summarise(
    sensitivity = sens_vec(class, pred_class),
    specificity = spec_vec(class, pred_class),
    accuracy = accuracy_vec(class, pred_class),
    roc_auc = roc_auc_vec(class, 1- pred)
  ) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3))) %>%
  bind_rows(tibble(
    model = "Hard Intersection",
    sensitivity = 0.812,
    specificity = 0.733,
    accuracy = 0.774,
    roc_auc = 0.883
  ))

all_metrics

```
The results show that the Fuzzy intersection model preforms better then the Hard intersection model. Yielding a sensitivity of 0.867, specificity of 0.875, accuracy of 0.871, and ROC AUC of 0.954 all of which are better than the hard intersection model. These values represent the fuzzy model being more balances and effective at distinguishing between ASD and TD subject without disproportionately favoring one group. The better performance is likely coming from its capacity to integrate partially overlapping information across selection methods(t-test, random forest, correlation test). By allowing proteins that appear in multiple but not all selection methods not risking dropping meaningful biomarkers that are emphasized by only some of the methods. 

```{r}
 # Original hard intersection proteins
proteins_core <- c("DERM", "RELT", "IgD", "FSTL1")
 
remaining_proteins <- setdiff(proteins_hard, proteins_core)
 
compute_metrics_seq <- function(core, add_protein) {
  proteins <- c(core, add_protein)
  data <- biomarker_clean %>%
    select(group, all_of(proteins)) %>%
    mutate(class = (group == "ASD")) %>%
    select(-group)
 
  res <- logistic(data, model_name = add_protein)
 
  tibble(
    protein_added = add_protein,
    auc = res$metrics %>% filter(.metric == "roc_auc") %>% pull(.estimate),
    sensitivity = res$metrics %>% filter(.metric == "sensitivity") %>% pull(.estimate),
    specificity = res$metrics %>% filter(.metric == "specificity") %>% pull(.estimate)
  )
}
 
# Apply sequentially for each remaining protein
metrics_seq <- map_dfr(remaining_proteins, ~compute_metrics_seq(proteins_core, .x))
 
metrics_seq <- metrics_seq %>% arrange(desc(auc))
 
metrics_seq
```
 
### Question 4
Use any method to find either:
a) A simpler panel that achieves comparable classification accuracy

```{r}
library(e1071)

test_2protein_svm <- function(protein1, protein2, data, seed = 101422) {
  combo_data <- data %>%
    select(group, all_of(c(protein1, protein2))) %>%
    mutate(class = factor(group == 'ASD')) %>%
    select(-group)
  
  set.seed(seed)
  combo_split <- combo_data %>% initial_split(prop = 0.8)
  
  train_combo <- training(combo_split) %>% mutate(class = factor(class))
  fit_svm <- svm(x = train_combo %>% select(-class),
                 y = train_combo$class,
                 kernel = "radial",
                 probability = TRUE)
  
  test_combo <- testing(combo_split) %>% select(-class)
  pred_svm <- predict(fit_svm, test_combo)
  pred_prob <- predict(fit_svm, test_combo, probability = TRUE)
  pred_prob_asd <- attr(pred_prob, "probabilities")[, "TRUE"]
  
  results <- testing(combo_split) %>%
    mutate(pred_class = pred_svm,
           pred_prob = pred_prob_asd,
           truth_class = factor(class)) %>%
    class_metrics(estimate = pred_class,
                  truth = truth_class, 
                  pred_prob,
                  event_level = 'second')
  
  return(results)
}

cat("\n=== RELT + DERM ===\n")
print(test_2protein_svm("RELT", "DERM", biomarker_clean))

```

b) An alternative panel that achieves improved classification accuracy

```{r}
library(glmnet)

predictors_lasso <- biomarker_clean %>%
  select(-c(group, ados)) %>%
  as.matrix()

response_lasso <- if_else(biomarker_clean$group == "ASD", 1, 0)

# partition into training and test set
set.seed(101422)
split_lasso <- initial_split(biomarker_clean, prop = 0.8)
train_data_lasso <- training(split_lasso)
test_data_lasso  <- testing(split_lasso)

x_train_lasso <- train_data_lasso %>% select(-c(group, ados)) %>% as.matrix()
y_train_lasso <- if_else(train_data_lasso$group == "ASD", 1, 0)

x_test_lasso <- test_data_lasso %>% select(-c(group, ados)) %>% as.matrix()
y_test_lasso <- if_else(test_data_lasso$group == "ASD", 1, 0)

#fit lasso regularization
set.seed(101422)
cvfit <- cv.glmnet(
  x_train_lasso, y_train_lasso,
  family = "binomial",
  alpha = 1,       # LASSO
  nfolds = 5
)

# lambda that minimizes cross-validated error
lambda_min <- cvfit$lambda.min

#importance
lasso_coefs <- coef(cvfit, s = "lambda.min")
proteins_lasso <- rownames(lasso_coefs)[lasso_coefs[,1] != 0]
proteins_lasso <- setdiff(proteins_lasso, "(Intercept)")

proteins_lasso

# predicted probabilities
test_data_lasso_pred <- test_data_lasso %>%
  mutate(
    pred = as.numeric(predict(cvfit, newx = x_test_lasso, s = lambda_min, type = "response")),
    preds = factor(if_else(pred > 0.3, "ASD", "TD"), levels = c("TD", "ASD")),
    classes = factor(if_else(y_test_lasso == 1, "ASD", "TD"), levels = c("TD", "ASD"))
  )

class_metrics <- metric_set(sensitivity, specificity, accuracy, roc_auc)

class_metrics(
  test_data_lasso_pred,
  estimate = preds,
  truth = classes,
  pred,
  event_level = "second"
)
```

