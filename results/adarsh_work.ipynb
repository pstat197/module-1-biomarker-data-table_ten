{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Analysis Modification - Dataset Testing Diffrentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pyreadr\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "data = pd.read_csv(\n",
    "  \"https://raw.githubusercontent.com/pstat197/module-1-biomarker-data-table_ten/refs/heads/main/data/biomarker-raw.csv\"\n",
    "  )\n",
    "\n",
    "# drop nuisance cols\n",
    "data.drop(0, inplace=True)\n",
    "data.drop(columns=[\"Target Full Name\"], inplace=True)\n",
    "\n",
    "# convert protein level data to float\n",
    "group_col = data[\"Group\"]\n",
    "data.drop(columns=[\"Group\"], inplace=True)\n",
    "data = data.replace('-', pd.NA)\n",
    "\n",
    "# Replace missing markers with NaN but don’t drop them\n",
    "data = data.replace('-', pd.NA)\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "data = data.astype(float)\n",
    "\n",
    "data.insert(0, \"Group\", group_col)\n",
    "biomarker_clean = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeat the analysis but carry out the entire selection procedure on a training partition -- in other words, set aside some testing data at the very beginning and don't use it until you are evaluating accuracy at the very end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (OOB) confusion:\n",
      " Pred  ASD  TD\n",
      "True         \n",
      "ASD    46  15\n",
      "TD     20  42 \n",
      "\n",
      "Training Confusion Matrix\n",
      "Pred  ASD  TD\n",
      "True         \n",
      "ASD    46  15\n",
      "TD     18  44\n",
      "Testing Confusion Matrix\n",
      "Pred  ASD  TD\n",
      "True         \n",
      "ASD    12   3\n",
      "TD      6  10\n"
     ]
    }
   ],
   "source": [
    "# --- Assumes your preprocessing just ran and you have: biomarker_clean = data.copy() ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.special import digamma\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, recall_score\n",
    "\n",
    "# --------------- Helpers ---------------\n",
    "\n",
    "def welch_t_two_sided(col, df, group_col='Group', order=('ASD','TD')):\n",
    "    \"\"\"Welch two-sample t-test; ignores NaNs.\"\"\"\n",
    "    g1, g2 = order\n",
    "    x = df.loc[df[group_col] == g1, col].astype(float)\n",
    "    y = df.loc[df[group_col] == g2, col].astype(float)\n",
    "    _, p = stats.ttest_ind(x, y, equal_var=False, nan_policy='omit')\n",
    "    return p\n",
    "\n",
    "def by_adjust(pvals):\n",
    "    \"\"\"Benjamini-Yekutieli adjustment using the same approximation your R used:\n",
    "       hm = log(m) + 1/(2m) - digamma(1)\n",
    "    \"\"\"\n",
    "    m = len(pvals)\n",
    "    hm = np.log(m) + 1.0/(2*m) - digamma(1)\n",
    "    ranks = np.arange(1, m+1, dtype=float)\n",
    "    p_sorted = np.array(pvals)\n",
    "    p_adj = np.minimum(1.0, (m * hm * p_sorted) / ranks)\n",
    "    return p_adj, hm\n",
    "\n",
    "# --------------- Dataset Manipulation + Split ---------------\n",
    "biomarker_fix = biomarker_clean.dropna(subset=['Group']).copy()\n",
    "biomarker_fix['Group'] = biomarker_fix['Group'].astype(str).str.strip()\n",
    "feature_cols = [c for c in biomarker_fix.columns if c != 'Group']\n",
    "X = biomarker_fix[feature_cols]\n",
    "y = biomarker_fix['Group']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=101422,\n",
    "    stratify=y \n",
    ")\n",
    "df_train = X_train.copy()\n",
    "df_train['Group'] = y_train.values\n",
    "\n",
    "# --------------- T-tests + BY ---------------\n",
    "\n",
    "# Compute p-values (Welch, two-sided), NaNs automatically omitted\n",
    "ttest_rows = []\n",
    "for protein in X_train.columns:\n",
    "    p = welch_t_two_sided(protein, df_train, 'Group', ('ASD','TD'))\n",
    "    ttest_rows.append({'protein': protein, 'p_value': p})\n",
    "\n",
    "ttests_out = (pd.DataFrame(ttest_rows)\n",
    "              .sort_values('p_value', ascending=True)\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "# BY correction (R-style approximation for harmonic number)\n",
    "m = len(ttests_out)\n",
    "p_adj, hm = by_adjust(ttests_out['p_value'].values)\n",
    "ttests_out['m'] = m\n",
    "ttests_out['hm'] = hm\n",
    "ttests_out['rank'] = np.arange(1, m+1)\n",
    "ttests_out['p_adj'] = p_adj\n",
    "\n",
    "# Top 10 by adjusted p-value\n",
    "proteins_s1 = ttests_out.nsmallest(10, 'p_adj')['protein'].tolist()\n",
    "\n",
    "# --------------- Random Forest (+ impute, OOB confusion) ---------------\n",
    "predictors = X_train.copy()\n",
    "response = y_train.copy()\n",
    "\n",
    "\n",
    "# Pipeline: impute (median) → RF with OOB enabled\n",
    "rf_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        oob_score=True,        # mirror R's rf_out$confusion using OOB\n",
    "        bootstrap=True,\n",
    "        random_state=101422,\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")\n",
    "rf_pipe.fit(predictors, response)\n",
    "\n",
    "# OOB preds → confusion\n",
    "rf = rf_pipe.named_steps['randomforestclassifier']\n",
    "if hasattr(rf, 'oob_decision_function_') and rf.oob_decision_function_ is not None:\n",
    "    oob_prob = rf.oob_decision_function_\n",
    "    oob_pred = rf.classes_[oob_prob.argmax(axis=1)]\n",
    "    cm_oob = pd.DataFrame(\n",
    "        confusion_matrix(response, oob_pred, labels=['ASD','TD']),\n",
    "        index=pd.Index(['ASD','TD'], name='True'),\n",
    "        columns=pd.Index(['ASD','TD'], name='Pred')\n",
    "    )\n",
    "    print(\"Random Forest (OOB) confusion:\\n\", cm_oob, \"\\n\")\n",
    "\n",
    "# Importances\n",
    "imp = (pd.DataFrame({\n",
    "            'protein': predictors.columns,\n",
    "            'importance': rf.feature_importances_\n",
    "       })\n",
    "       .sort_values('importance', ascending=False))\n",
    "proteins_s2 = imp.head(10)['protein'].tolist()\n",
    "\n",
    "# --------------- Logistic Regression on intersection (+ impute) ---------------\n",
    "\n",
    "proteins_sstar = sorted(set(proteins_s1).intersection(set(proteins_s2)))\n",
    "#if not proteins_sstar:\n",
    "#   # widen to 20 if empty to avoid dead-end\n",
    "#    proteins_s1 = ttests_out.nsmallest(20, 'p_adj')['protein'].tolist()\n",
    "#    proteins_s2 = imp.head(20)['protein'].tolist()\n",
    "#    proteins_sstar = sorted(set(proteins_s1).intersection(set(proteins_s2)))\n",
    "\n",
    "## Use the SAME original split with the selected features\n",
    "Xtr = X_train[proteins_sstar]\n",
    "Xte = X_test[proteins_sstar]\n",
    "ytr = (y_train == 'ASD').astype(int)\n",
    "yte = (y_test == 'ASD').astype(int)\n",
    "\n",
    "# Pipeline: impute (median) → logistic regression\n",
    "logit_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    "    LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    ")\n",
    "\n",
    "logit_pipe.fit(Xtr, ytr)\n",
    "\n",
    "#Test Probs\n",
    "probs = logit_pipe.predict_proba(Xte)[:, 1]\n",
    "pred = (probs > 0.5).astype(int)\n",
    "\n",
    "#Train Probs\n",
    "probs_train = logit_pipe.predict_proba(Xtr)[:, 1]\n",
    "train_pred = (probs_train > 0.5).astype(int)\n",
    "\n",
    "cm_train = pd.DataFrame(\n",
    "    confusion_matrix(ytr, train_pred, labels=[1, 0]),\n",
    "    index=pd.Index(['ASD', 'TD'], name='True'),\n",
    "    columns=pd.Index(['ASD', 'TD'], name='Pred')\n",
    ")\n",
    "cm_test = pd.DataFrame(\n",
    "    confusion_matrix(yte, pred, labels=[1, 0]),\n",
    "    index=pd.Index(['ASD', 'TD'], name='True'),\n",
    "    columns=pd.Index(['ASD', 'TD'], name='Pred')\n",
    ")\n",
    "print(\"Training Confusion Matrix\")\n",
    "print(cm_train)\n",
    "print(\"Testing Confusion Matrix\")\n",
    "print(cm_test)\n",
    "\n",
    "#test metrics\n",
    "tn, fp, fn, tp = confusion_matrix(yte, pred).ravel()\n",
    "accuracy = accuracy_score(yte, pred)\n",
    "sensitivity = recall_score(yte, pred)  # ASD==1\n",
    "specificity = tn / (tn + fp) if (tn + fp) else np.nan\n",
    "roc_auc = roc_auc_score(yte, probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
